{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "\n",
    "1. Data processing and manipulation\n",
    "2. Model training with initial setting\n",
    "3. RandomizedSearchCv to search for best hyper parameter\n",
    "4. Model training with tuned setting\n",
    "5. Model error comparison between initial setting and tuned hyper-parameter model\n",
    "6. Tuned-model feature importance visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1:Data processing and manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import janestreet\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from xgboost import XGBRegressor# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "chunksize = 10 ** 6\n",
    "filename = r'/kaggle/input/jane-street-market-prediction/train.csv'\n",
    "data_chunk = []\n",
    "start_time = time.time()\n",
    "data_chunk = pd.read_csv(filename)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Start with processing the label , since evaluation is only fixed to either buy or pass\n",
    "\n",
    "Define a function to return either 0 = Pass , 1 = Buy \n",
    "\n",
    "**Buy = weight * resp > 0**\n",
    "\n",
    "**Pass = weight * resp <= 0** \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buy_or_pass(df):\n",
    "    if df['action'] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process on feature and action for model learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data mentioned the return will be based on weight and resp columns . create another column call return \n",
    "def feature_action_split(dataframe_market):\n",
    "    '''\n",
    "    Input : Sample dataframe from Jane market prediction data\n",
    "    Output : feature = not response , weight , date or ts_id\n",
    "             action = 0 for pass and 1 for buy\n",
    "    '''\n",
    "#     dataframe_market = dataframe_market[dataframe_market['weight'] > 0]\n",
    "    dataframe_market['action'] = dataframe_market['weight']*dataframe_market['resp']\n",
    "    dataframe_market['action'] = dataframe_market.apply(buy_or_pass,axis=1) \n",
    "    feature = dataframe_market.drop(['date','weight','resp_1','resp_2','resp_3','resp_4','resp','ts_id','action'],axis=1)\n",
    "    print(\"Features columns : \",feature.columns)\n",
    "    action = dataframe_market[['action']]\n",
    "    print(\"Action counts : \\n\",action.value_counts())\n",
    "    return feature,action\n",
    "data_chunk = data_chunk[data_chunk['weight'] > 0]\n",
    "feature,action = feature_action_split(data_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_transform = ((data_chunk['weight'].values * data_chunk['resp'].values) > 0).astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 : Model training with initial setting\n",
    "\n",
    "Model training . Imputer is not necessary needed for xgboost.\n",
    "\n",
    "With basic model , we fixed the params to run.\n",
    "\n",
    "If you are running xgboost on gpu , enable tree_method = 'gpu_hist' else run on normal CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(feature, action.values.flatten(), test_size=0.25) # By default shuffle is true\n",
    "\n",
    "my_imputer = SimpleImputer()\n",
    "train_X = my_imputer.fit_transform(train_X)\n",
    "test_X = my_imputer.transform(test_X)\n",
    "start_time = time.time()\n",
    "\n",
    "initial_model = xgb.XGBClassifier(n_estimators=1000, \n",
    "                        max_depth=5, \n",
    "                        learning_rate=0.1, \n",
    "                        subsample=0.7,\n",
    "                        colsample_bytree=0.8, \n",
    "                        colsample_bylevel=0.8, \n",
    "                        base_score=train_y.mean(),\n",
    "                        tree_method= 'gpu_hist',\n",
    "                        random_state=42, seed=42)\n",
    "\n",
    "init_mod = initial_model.fit(train_X, train_y, \n",
    "                    early_stopping_rounds=10, \n",
    "                    eval_set=[(test_X, test_y)], eval_metric='error', \n",
    "                    verbose=100)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['pass','buy']\n",
    "\n",
    "disp = plot_confusion_matrix(init_mod, test_X, test_y,\n",
    "                             display_labels=class_names,\n",
    "                             cmap=plt.cm.Blues)\n",
    "plt.title('Initial Model without tuning using Xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial result is decent where test data of 250k points. the losses will incurred when it's predicted __buy__ and it's actual __pass__.\n",
    "\n",
    "Ideal case for the market is to minimize losses where prediction false positive where true label is pass but predicted as buy and secondly maximizing profit where true positive is predicted buy and true label as buy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Part 3: Model tuning using RandomizedSearchCv to search for best hyper parameter\n",
    "\n",
    "\n",
    "\n",
    "This run will take time , it took 155 minutes to run over 1500 fits with GPU on. \n",
    "* Please make sure you on GPU setting when you are running this.\n",
    "\n",
    "__This part will be commented out as it will take approximately 150 minutes to run__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# params = {\n",
    "#         'learning_rate': [0.03, 0.01, 0.003, 0.001],\n",
    "#         'min_child_weight': [1,3, 5,7, 10],\n",
    "#         'gamma': [0, 0.5, 1, 1.5, 2, 2.5, 5],\n",
    "#         'subsample': [0.6, 0.8, 1.0, 1.2, 1.4],\n",
    "#         'colsample_bytree': [0.6, 0.8, 1.0, 1.2, 1.4],\n",
    "#         'max_depth': [3, 4, 5, 6, 7, 8, 9 ,10, 12, 14],\n",
    "#         'reg_lambda':np.array([0.4, 0.6, 0.8, 1, 1.2, 1.4])}\n",
    "\n",
    "# # specific parameters. I set early stopping to avoid overfitting and specify the validation dataset \n",
    "# fit_params = { \n",
    "#         'early_stopping_rounds':10,\n",
    "#         'eval_set':[(test_X, test_y)]}\n",
    "\n",
    "# # let's run the optimization\n",
    "# random_search = RandomizedSearchCV(init_mod, param_distributions=params, n_iter=500,\n",
    "#                                    scoring=\"precision\", n_jobs=-1,  verbose=3, random_state=42, cv=3 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_search.fit(train_X,train_y, **fit_params)\n",
    "# print(\" Results from Random Search \" )\n",
    "# print(\"\\n The best estimator across ALL searched params:\\n\", random_search.best_estimator_)\n",
    "# print(\"\\n The best score across ALL searched params:\\n\", random_search.best_score_)\n",
    "# print(\"\\n The best parameters across ALL searched params:\\n\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4:  Model training with best hyper-parameter result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(feature, action.values.flatten(), test_size=0.25) # By default shuffle is true\n",
    "\n",
    "my_imputer = SimpleImputer()\n",
    "train_X = my_imputer.fit_transform(train_X)\n",
    "test_X = my_imputer.transform(test_X)\n",
    "start_time = time.time()\n",
    "\n",
    "tuned_model = xgb.XGBClassifier(n_estimators=1000, \n",
    "                        max_depth=14, \n",
    "                        learning_rate=0.01, \n",
    "                        subsample=1,\n",
    "                        colsample_bytree=0.8, \n",
    "                        colsample_bylevel=0.8, \n",
    "                        gamma=0.5,reg_lambda = 1.4,\n",
    "                        base_score=train_y.mean(),\n",
    "                        tree_method= 'gpu_hist',\n",
    "                        random_state=42, seed=42)\n",
    "\n",
    "\n",
    "tuned_mod = tuned_model.fit(train_X, train_y, \n",
    "                    early_stopping_rounds=10, \n",
    "                    eval_set=[(test_X, test_y)], eval_metric='error', \n",
    "                    verbose=100)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['pass','buy']\n",
    "\n",
    "disp = plot_confusion_matrix(tuned_mod, test_X, test_y,\n",
    "                             display_labels=class_names,\n",
    "                             cmap=plt.cm.Blues)\n",
    "plt.title('Tuned Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Model error comparison between initial Xgboost classifer model and tuned hyperparameter xgboost classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "init_model_res = init_mod.evals_result()\n",
    "tuned_model_res = tuned_mod.evals_result()\n",
    "epochs_init = len(init_model_res['validation_0']['error'])\n",
    "epochs_tuned = len(tuned_model_res['validation_0']['error'])\n",
    "x_axis_init = range(0, epochs_init)\n",
    "x_axis_tuned = range(0,epochs_tuned)\n",
    "# plot classification error\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_axis_init, init_model_res['validation_0']['error'], label='Initial classification error')\n",
    "ax.plot(x_axis_tuned, tuned_model_res['validation_0']['error'], label='Tuned classification error')\n",
    "\n",
    "ax.legend()\n",
    "plt.ylabel('Classification Error')\n",
    "plt.title('XGBoost Classification error on test data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Tuned-model feature importance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance\n",
    "print(tuned_mod.get_booster().get_score(importance_type='weight'))\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(15,15))\n",
    "plot_importance(tuned_mod,ax=ax,max_num_features=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.iloc[:,2:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = janestreet.make_env() # initialize the environment\n",
    "iter_test = env.iter_test() # an iterator which loops over the test set\n",
    "\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    sample_prediction_df.action = tuned_mod.predict(test_df.iloc[:,2:].values)\n",
    "    env.predict(sample_prediction_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
